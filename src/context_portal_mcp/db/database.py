"""Database interaction logic using sqlite3."""

import sqlite3
import json
import os
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime, timedelta, timezone

from alembic.config import Config
from alembic import command
import logging

from ..core.config import get_database_path
from ..core.exceptions import DatabaseError, ConfigurationError
from . import models # Import models from the same directory
import shutil # For copying directories
import inspect

log = logging.getLogger(__name__) # Get a logger for this module

# --- SQLite datetime adapters/converters (UTC) ---
def _adapt_datetime(dt: datetime) -> str:
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    else:
        dt = dt.astimezone(timezone.utc)
    return dt.isoformat(timespec="seconds")

def _convert_datetime(b: bytes) -> datetime:
    s = b.decode()
    try:
        dt = datetime.fromisoformat(s)
    except ValueError:
        # Fallback for SQLite CURRENT_TIMESTAMP format 'YYYY-MM-DD HH:MM:SS'
        dt = datetime.fromisoformat(s.replace(" ", "T"))
    if dt.tzinfo is None:
        return dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc)

sqlite3.register_adapter(datetime, _adapt_datetime)
sqlite3.register_converter("DATETIME", _convert_datetime)
sqlite3.register_converter("TIMESTAMP", _convert_datetime)

# --- Alembic File Content Constants ---

ALEMBIC_INI_CONTENT = """
# A generic Alembic configuration file.

[alembic]
# path to migration scripts
script_location = alembic

# The database URL is now set dynamically by ConPort's run_migrations function.
# sqlalchemy.url = sqlite:///your_database.db
# ... other Alembic settings ...
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
"""

ENV_PY_CONTENT = """
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line prevents the need to have a separate logging config file.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = None

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    \"\"\"Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    \"\"\"
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    \"\"\"Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    \"\"\"
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
"""

INITIAL_SCHEMA_CONTENT = """
\"\"\"Initial schema

Revision ID: 20250617
Revises:
Create Date: 2025-06-17 15:00:00.000000

\"\"\"
from alembic import op
import sqlalchemy as sa
import json

# revision identifiers, used by Alembic.
revision = '20250617'
down_revision = None
branch_labels = None
depends_on = None


def upgrade() -> None:
    # ### commands auto-generated by Alembic - please adjust! ###
    op.create_table('active_context',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('content', sa.Text(), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('active_context_history',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('timestamp', sa.DateTime(), nullable=False),
    sa.Column('version', sa.Integer(), nullable=False),
    sa.Column('content', sa.Text(), nullable=False),
    sa.Column('change_source', sa.String(length=255), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('context_links',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('workspace_id', sa.String(length=1024), nullable=False),
    sa.Column('source_item_type', sa.String(length=255), nullable=False),
    sa.Column('source_item_id', sa.String(length=255), nullable=False),
    sa.Column('target_item_type', sa.String(length=255), nullable=False),
    sa.Column('target_item_id', sa.String(length=255), nullable=False),
    sa.Column('relationship_type', sa.String(length=255), nullable=False),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('timestamp', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_context_links_source_item_id'), 'context_links', ['source_item_id'], unique=False)
    op.create_index(op.f('ix_context_links_source_item_type'), 'context_links', ['source_item_type'], unique=False)
    op.create_index(op.f('ix_context_links_target_item_id'), 'context_links', ['target_item_id'], unique=False)
    op.create_index(op.f('ix_context_links_target_item_type'), 'context_links', ['target_item_type'], unique=False)
    op.create_table('custom_data',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('timestamp', sa.DateTime(), nullable=False),
    sa.Column('category', sa.String(length=255), nullable=False),
    sa.Column('key', sa.String(length=255), nullable=False),
    sa.Column('value', sa.Text(), nullable=False),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('category', 'key')
    )
    op.create_table('decisions',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('timestamp', sa.DateTime(), nullable=False),
    sa.Column('summary', sa.Text(), nullable=False),
    sa.Column('rationale', sa.Text(), nullable=True),
    sa.Column('implementation_details', sa.Text(), nullable=True),
    sa.Column('tags', sa.Text(), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('product_context',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('content', sa.Text(), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('product_context_history',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('timestamp', sa.DateTime(), nullable=False),
    sa.Column('version', sa.Integer(), nullable=False),
    sa.Column('content', sa.Text(), nullable=False),
    sa.Column('change_source', sa.String(length=255), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('progress_entries',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('timestamp', sa.DateTime(), nullable=False),
    sa.Column('status', sa.String(length=50), nullable=False),
    sa.Column('description', sa.Text(), nullable=False),
    sa.Column('parent_id', sa.Integer(), nullable=True),
    sa.ForeignKeyConstraint(['parent_id'], ['progress_entries.id'], ondelete='SET NULL'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('system_patterns',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('timestamp', sa.DateTime(), nullable=False),
    sa.Column('name', sa.String(length=255), nullable=False),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('tags', sa.Text(), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('name')
    )
    
    # Seed initial data
    op.execute("INSERT INTO product_context (id, content) VALUES (1, '{}')")
    op.execute("INSERT INTO active_context (id, content) VALUES (1, '{}')")

    # Create FTS5 virtual table for decisions (guarded)
    try:
        op.execute('''
        CREATE VIRTUAL TABLE decisions_fts USING fts5(
            summary,
            rationale,
            implementation_details,
            tags,
            content="decisions",
            content_rowid="id"
        );
        ''')

        # Create triggers to keep the FTS table in sync with the decisions table
        op.execute('''
        CREATE TRIGGER decisions_after_insert AFTER INSERT ON decisions
        BEGIN
            INSERT INTO decisions_fts (rowid, summary, rationale, implementation_details, tags)
            VALUES (new.id, new.summary, new.rationale, new.implementation_details, new.tags);
        END;
        ''')
        op.execute('''
        CREATE TRIGGER decisions_after_delete AFTER DELETE ON decisions
        BEGIN
            INSERT INTO decisions_fts (decisions_fts, rowid, summary, rationale, implementation_details, tags)
            VALUES ('delete', old.id, old.summary, old.rationale, old.implementation_details, old.tags);
        END;
        ''')
        op.execute('''
        CREATE TRIGGER decisions_after_update AFTER UPDATE ON decisions
        BEGIN
            INSERT INTO decisions_fts (decisions_fts, rowid, summary, rationale, implementation_details, tags)
            VALUES ('delete', old.id, old.summary, old.rationale, old.implementation_details, old.tags);
            INSERT INTO decisions_fts (rowid, summary, rationale, implementation_details, tags)
            VALUES (new.id, new.summary, new.rationale, new.implementation_details, new.tags);
        END;
        ''')
    except Exception as e:
        log.debug(f"Warning: decisions FTS5 not created: {e}")

    # Create FTS5 virtual table for custom_data (guarded)
    try:
        op.execute('''
        CREATE VIRTUAL TABLE custom_data_fts USING fts5(
            category,
            key,
            value_text,
            content="custom_data",
            content_rowid="id"
        );
        ''')

        # Create triggers for custom_data_fts
        op.execute('''
        CREATE TRIGGER custom_data_after_insert AFTER INSERT ON custom_data
        BEGIN
            INSERT INTO custom_data_fts (rowid, category, key, value_text)
            VALUES (new.id, new.category, new.key, new.value);
        END;
        ''')
        op.execute('''
        CREATE TRIGGER custom_data_after_delete AFTER DELETE ON custom_data
        BEGIN
            INSERT INTO custom_data_fts (custom_data_fts, rowid, category, key, value_text)
            VALUES ('delete', old.id, old.category, old.key, old.value);
        END;
        ''')
        op.execute('''
        CREATE TRIGGER custom_data_after_update AFTER UPDATE ON custom_data
        BEGIN
            INSERT INTO custom_data_fts (custom_data_fts, rowid, category, key, value_text)
            VALUES ('delete', old.id, old.category, old.key, old.value);
            INSERT INTO custom_data_fts (rowid, category, key, value_text)
            VALUES (new.id, new.category, new.key, new.value);
        END;
        ''')
    except Exception as e:
        log.debug(f"Warning: custom_data FTS5 not created: {e}")

    # Note: progress_entries_fts, system_patterns_fts, and context_fts
    # are added in migration 2025_08_15_add_missing_fts_tables.py
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto-generated by Alembic - please adjust! ###
    op.drop_table('system_patterns')
    op.drop_table('progress_entries')
    op.drop_table('product_context_history')
    op.drop_table('product_context')
    op.drop_table('decisions')
    op.drop_table('custom_data')
    op.drop_index(op.f('ix_context_links_target_item_type'), table_name='context_links')
    op.drop_index(op.f('ix_context_links_target_item_id'), table_name='context_links')
    op.drop_index(op.f('ix_context_links_source_item_type'), table_name='context_links')
    op.drop_index(op.f('ix_context_links_source_item_id'), table_name='context_links')
    op.drop_table('context_links')
    op.drop_table('active_context_history')
    op.drop_table('active_context')
    # ### end Alembic commands ###
"""
 
# Import templates from separate file to keep database.py manageable
from .templates import LAUNCHER_PY_TEMPLATE, KILLER_PY_TEMPLATE, ADD_MISSING_FTS_TABLES_CONTENT, FTS_ROWID_CONFLICT_FIX_CONTENT, LINK_CLEANUP_TRIGGERS_CONTENT, FIX_CUSTOM_DATA_FTS_COLUMN_CONTENT
 
# --- Connection Handling ---
 
_connections: Dict[str, sqlite3.Connection] = {}
 
def get_db_connection(workspace_id: str) -> sqlite3.Connection:
    """
    Gets or creates a database connection for the given workspace.
    This function now orchestrates the entire workspace initialization on first call.
    """
    if workspace_id in _connections:
        return _connections[workspace_id]

    # 1. Ensure all necessary directories and Alembic files are present.
    # This is the core of the deferred initialization.
    ensure_alembic_files_exist(workspace_id)

    # 2. Get the database path (which should now exist within the created directories).
    db_path = get_database_path(workspace_id)
    
    # 3. Run migrations to create/update the database schema.
    run_migrations(db_path)

    # 4. Establish and cache the database connection.
    try:
        conn = sqlite3.connect(db_path, detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES, timeout=30.0)
        conn.row_factory = sqlite3.Row # Access columns by name
        
        # Enable WAL mode for concurrent access between MCP and HTTP servers
        cursor = conn.cursor()
        cursor.execute("PRAGMA journal_mode=WAL;")
        cursor.execute("PRAGMA busy_timeout=30000;")  # 30 second timeout for lock contention
        cursor.execute("PRAGMA synchronous=NORMAL;")  # Faster writes while maintaining safety
        conn.commit()
        log.info(f"Enabled WAL mode for concurrent database access: {db_path}")
        
        _connections[workspace_id] = conn
        log.info(f"Successfully initialized and connected to database for workspace: {workspace_id}")
        return conn
    except ConfigurationError as e:
        log.error(f"Configuration error during DB connection for {workspace_id}: {e}")
        raise DatabaseError(f"Configuration error getting DB path for {workspace_id}: {e}")
    except sqlite3.Error as e:
        log.error(f"SQLite error during DB connection for {workspace_id} at {db_path}: {e}")
        raise DatabaseError(f"Failed to connect to database for {workspace_id} at {db_path}: {e}")

def close_db_connection(workspace_id: str):
    """Closes the database connection for the given workspace, if open."""
    if workspace_id in _connections:
        _connections[workspace_id].close()
        del _connections[workspace_id]

def close_all_connections():
    """Closes all active database connections."""
    for workspace_id in list(_connections.keys()):
        close_db_connection(workspace_id)

# --- Alembic Migration Integration ---

def ensure_alembic_files_exist(workspace_id: str):
    """
    Ensures that alembic.ini and the alembic/ directory exist within the
    database directory. If not, copies them from the
    server's internal templates.
    """
    # The actual directory where context_aimed.db resides and where Alembic files should be
    # Get the database path to determine where Alembic files should be located
    from ..core.config import get_database_path
    db_path = get_database_path(workspace_id)
    conport_db_dir = db_path.parent
    conport_db_dir.mkdir(exist_ok=True, parents=True) # Ensure this directory exists

    alembic_ini_path = conport_db_dir / "alembic.ini"
    alembic_dir_path = conport_db_dir / "alembic"

    # Check for alembic.ini
    if not alembic_ini_path.exists():
        log.info(f"Alembic.ini not found. Creating at {alembic_ini_path}")
        try:
            with open(alembic_ini_path, 'w') as f:
                f.write(ALEMBIC_INI_CONTENT)
        except IOError as e:
            log.error(f"Failed to write alembic.ini at {alembic_ini_path}: {e}")
            raise DatabaseError(f"Could not create alembic.ini: {e}")

    # Check for alembic/env.py
    alembic_env_py_path = alembic_dir_path / "env.py"
    if not alembic_env_py_path.exists():
        log.info(f"Alembic env.py not found. Creating at {alembic_env_py_path}")
        try:
            # ensure parent directory exists
            os.makedirs(alembic_dir_path, exist_ok=True)
            with open(alembic_env_py_path, 'w') as f:
                f.write(ENV_PY_CONTENT)
        except IOError as e:
            log.error(f"Failed to write env.py at {alembic_env_py_path}: {e}")
            raise DatabaseError(f"Could not create env.py: {e}")

    # Check for alembic/versions directory and initial schema
    alembic_versions_path = alembic_dir_path / "versions"
    initial_schema_path = alembic_versions_path / "2025_06_17_initial_schema.py"
    if not initial_schema_path.exists():
        log.info(f"Initial schema not found. Creating at {initial_schema_path}")
        try:
            os.makedirs(alembic_versions_path, exist_ok=True)
            with open(initial_schema_path, 'w') as f:
                f.write(INITIAL_SCHEMA_CONTENT)
        except OSError as e:
            log.error(f"Failed to create initial schema at {initial_schema_path}: {e}")
            raise DatabaseError(f"Could not create initial schema: {e}")

    # Check for missing FTS migration
    missing_fts_path = alembic_versions_path / "2025_08_15_add_missing_fts_tables.py"
    if not missing_fts_path.exists():
        log.info(f"Missing FTS migration not found. Creating at {missing_fts_path}")
        try:
            os.makedirs(alembic_versions_path, exist_ok=True)
            with open(missing_fts_path, 'w') as f:
                f.write(ADD_MISSING_FTS_TABLES_CONTENT)
        except OSError as e:
            log.error(f"Failed to create missing FTS migration at {missing_fts_path}: {e}")
            raise DatabaseError(f"Could not create missing FTS migration: {e}")

    # Check for FTS rowid conflict fix migration
    fts_rowid_fix_path = alembic_versions_path / "2025_10_09_fix_context_fts_rowid_conflict.py"
    if not fts_rowid_fix_path.exists():
        log.info(f"FTS rowid conflict fix migration not found. Creating at {fts_rowid_fix_path}")
        try:
            os.makedirs(alembic_versions_path, exist_ok=True)
            with open(fts_rowid_fix_path, 'w') as f:
                f.write(FTS_ROWID_CONFLICT_FIX_CONTENT)
        except OSError as e:
            log.error(f"Failed to create FTS rowid conflict fix migration at {fts_rowid_fix_path}: {e}")
            raise DatabaseError(f"Could not create FTS rowid conflict fix migration: {e}")

    # Check for link cleanup triggers migration
    link_cleanup_path = alembic_versions_path / "2025_10_11_add_link_cleanup_triggers.py"
    if not link_cleanup_path.exists():
        log.info(f"Link cleanup triggers migration not found. Creating at {link_cleanup_path}")
        try:
            os.makedirs(alembic_versions_path, exist_ok=True)
            with open(link_cleanup_path, 'w') as f:
                f.write(LINK_CLEANUP_TRIGGERS_CONTENT)
        except OSError as e:
            log.error(f"Failed to create link cleanup triggers migration at {link_cleanup_path}: {e}")
            raise DatabaseError(f"Could not create link cleanup triggers migration: {e}")

    # Check for custom_data_fts column name fix migration
    fix_custom_data_fts_path = alembic_versions_path / "2025_10_22_fix_custom_data_fts_column_name.py"
    if not fix_custom_data_fts_path.exists():
        log.info(f"Custom data FTS column fix migration not found. Creating at {fix_custom_data_fts_path}")
        try:
            os.makedirs(alembic_versions_path, exist_ok=True)
            with open(fix_custom_data_fts_path, 'w') as f:
                f.write(FIX_CUSTOM_DATA_FTS_COLUMN_CONTENT)
        except OSError as e:
            log.error(f"Failed to create custom data FTS column fix migration at {fix_custom_data_fts_path}: {e}")
            raise DatabaseError(f"Could not create custom data FTS column fix migration: {e}")

    # Ensure portal_launcher.py exists and is valid (not empty or corrupted)
    launcher_path = conport_db_dir / "portal_launcher.py"
    launcher_needs_creation = True
    
    if launcher_path.exists():
        try:
            # Check if file is empty or too small (less than 100 bytes indicates corruption)
            if launcher_path.stat().st_size > 100:
                launcher_needs_creation = False
                log.debug(f"portal_launcher.py exists and is valid at {launcher_path}")
            else:
                log.info(f"portal_launcher.py exists but is empty or corrupted. Recreating at {launcher_path}")
        except Exception as e:
            log.warning(f"Failed to check launcher file size: {e}. Recreating launcher.")
    
    if launcher_needs_creation:
        try:
            log.info(f"Creating portal_launcher.py at {launcher_path}")
            
            # FIRST: Determine workspace_id from launcher location
            # The workspace_id is the absolute path of the parent directory of where we're writing the launcher
            actual_workspace_id = conport_db_dir.parent.resolve().as_posix()
            log.info(f"Determined workspace_id from launcher location: {actual_workspace_id}")
            
            # SECOND: Create env_vars.json ONLY if it doesn't exist (CRITICAL FIX Progress 85/86)
            # main.py sets central_python_executable BEFORE calling get_db_connection()
            # We must NOT overwrite env_vars.json if it already exists!
            try:
                import sys
                import json  # CRITICAL FIX: Import OUTSIDE the if block so it's available later
                
                ui_cache_dir = conport_db_dir / "ui-cache"
                ui_cache_dir.mkdir(exist_ok=True, parents=True)
                
                env_vars_file = ui_cache_dir / "env_vars.json"
                
                # ONLY create env_vars.json if it doesn't exist
                # If it exists, main.py has already set central_python_executable correctly
                if not env_vars_file.exists():
                    central_python_exe = sys.executable
                    
                    initial_env_vars = {
                        "workspace_id": actual_workspace_id,
                        "mcp_server_port": 8020,  # Default port, will be updated at runtime
                        "ui_port": 3000,  # Default port, will be updated at runtime
                        "conport_server_url": "http://localhost:8020/mcp/",  # Default, will be updated for WSL2
                        "wsl2_ip": None,
                        "wsl2_gateway_ip": None,
                        "central_python_executable": central_python_exe  # Captured during launcher creation
                    }
                    
                    # Write env_vars.json directly (avoid circular import by not using ui_cache functions)
                    with open(env_vars_file, 'w') as f:
                        json.dump(initial_env_vars, f, indent=2)
                    
                    log.info(f"Created initial env_vars.json with workspace_id: {actual_workspace_id}")
                    log.info(f"Captured central Python executable during launcher creation: {central_python_exe}")
                else:
                    log.info(f"env_vars.json already exists - NOT overwriting (preserves central_python_executable from main.py)")
                
            except Exception as env_e:
                log.error(f"Failed to create initial env_vars.json: {env_e}")
                # Continue anyway - launcher will fall back to runtime detection
            
            # THIRD: Create the launcher file
            with open(launcher_path, 'w', encoding='utf-8') as f:
                template = LAUNCHER_PY_TEMPLATE.lstrip('\n')
                
                # FIXED: Detect central AIMED UI path using THIS server's __file__ location
                # KEY INSIGHT: If this server is running, it has a valid environment!
                # We just record WHERE this running server code is located.
                ui_abs_path = ""
                
                try:
                    # Strategy 1: Use __file__ to find where THIS database.py code lives
                    # database.py is at: .../context-portal/src/context_portal_mcp/db/database.py
                    # Navigate up 4 levels to get context-portal/, then add /ui
                    current_script = Path(__file__).resolve()
                    conport_root = current_script.parent.parent.parent.parent
                    derived_ui_path = conport_root / "ui"
                    
                    if derived_ui_path.exists() and derived_ui_path.is_dir():
                        ui_abs_path = derived_ui_path.resolve().as_posix()
                        log.info(f"✓ Found central AIMED UI at: {ui_abs_path}")
                        log.info(f"  (Server root: {conport_root})")
                    else:
                        log.warning(f"UI directory not found at expected location: {derived_ui_path}")
                        
                except Exception as e:
                    log.warning(f"Failed to detect central UI path from __file__: {e}")
                
                # Strategy 2: Environment variable override (if user wants explicit path)
                if not ui_abs_path:
                    env_ui_path = os.getenv("CONPORT_UI_ABS_DIR")
                    if env_ui_path:
                        env_path = Path(env_ui_path).expanduser().resolve()
                        if env_path.exists() and env_path.is_dir():
                            ui_abs_path = str(env_path)
                            log.info(f"✓ Using AIMED UI from environment variable: {ui_abs_path}")
                
                if not ui_abs_path:
                    error_msg = (
                        f"AIMED launcher creation failed: Could not locate AIMED UI directory.\n"
                        f"Server code location: {Path(__file__).resolve()}\n"
                        f"Expected UI at: {Path(__file__).resolve().parent.parent.parent.parent / 'ui'}\n"
                        f"Please ensure AIMED is running from a repository checkout with ui/ directory."
                    )
                    log.error(error_msg)
                    raise DatabaseError(error_msg)
                
                # DECISION 100 FIX: Read central Python path from env_vars.json
                conport_python_path = ""
                try:
                    env_vars_file = ui_cache_dir / "env_vars.json"
                    if env_vars_file.exists():
                        with open(env_vars_file, 'r') as env_f:
                            env_data = json.load(env_f)
                            log.debug(f"env_vars.json raw content: {env_data}")
                            
                            # Handle both formats: {"data": {...}} or direct {...}
                            env_vars = env_data.get("data", env_data) if isinstance(env_data, dict) else {}
                            log.debug(f"Extracted env_vars dict: {env_vars}")
                            
                            conport_python_path = env_vars.get("central_python_executable", "")
                            if conport_python_path and conport_python_path != "":
                                log.info(f"✓ Read central Python path for launcher: {conport_python_path}")
                            else:
                                log.warning(f"central_python_executable empty or not found in env_vars.json - value was: '{conport_python_path}'")
                                log.warning(f"env_vars keys: {list(env_vars.keys())}")
                    else:
                        log.warning(f"env_vars.json doesn't exist at {env_vars_file}")
                except Exception as python_e:
                    log.warning(f"Failed to read central_python_executable from env_vars.json: {python_e}")
                    import traceback
                    log.warning(f"Traceback: {traceback.format_exc()}")
                
                # Populate the template with all required paths
                content = template.replace('{aimed_ui_path}', ui_abs_path)
                content = content.replace('{workspace_id}', actual_workspace_id)
                content = content.replace('{conport_python_path}', conport_python_path)  # Add central Python path
                f.write(content)
                f.flush()  # Ensure content is written before chmod
                
            log.info(f"Successfully created portal_launcher.py ({len(content)} bytes)")
            
            # Attempt to make it executable
            try:
                os.chmod(launcher_path, 0o755)
            except Exception as chmod_e:
                log.warning(f"Failed to chmod +x for {launcher_path}: {chmod_e}")
                
        except Exception as e:
            log.error(f"Failed to create portal_launcher.py at {launcher_path}: {e}", exc_info=True)
            # Clean up failed file to avoid empty files
            try:
                if launcher_path.exists():
                    launcher_path.unlink()
                    log.debug(f"Cleaned up failed launcher file at {launcher_path}")
            except Exception:
                pass

    # Ensure portal_killer.py exists and is valid (not empty or corrupted)
    killer_path = conport_db_dir / "portal_killer.py"
    killer_needs_creation = True
    
    if killer_path.exists():
        try:
            # Check if file is empty or too small (less than 100 bytes indicates corruption)
            if killer_path.stat().st_size > 100:
                killer_needs_creation = False
                log.debug(f"portal_killer.py exists and is valid at {killer_path}")
            else:
                log.info(f"portal_killer.py exists but is empty or corrupted. Recreating at {killer_path}")
        except Exception as e:
            log.warning(f"Failed to check killer file size: {e}. Recreating killer.")
    
    if killer_needs_creation:
        try:
            log.info(f"Creating portal_killer.py at {killer_path}")
            with open(killer_path, 'w', encoding='utf-8') as f:
                template = KILLER_PY_TEMPLATE.lstrip('\n')
                f.write(template)
                f.flush()  # Ensure content is written before chmod
                
            log.info(f"Successfully created portal_killer.py ({len(template)} bytes)")
            
            # Attempt to make it executable
            try:
                os.chmod(killer_path, 0o755)
            except Exception as chmod_e:
                log.warning(f"Failed to chmod +x for {killer_path}: {chmod_e}")
                
        except Exception as e:
            log.error(f"Failed to create portal_killer.py at {killer_path}: {e}", exc_info=True)
            # Clean up failed file to avoid empty files
            try:
                if killer_path.exists():
                    killer_path.unlink()
                    log.debug(f"Cleaned up failed killer file at {killer_path}")
            except Exception:
                pass

    # PLUGGABLE FEATURE: Create aimed_sync.py if template available (Decision 108)
    # This enables Git-based collaboration without making it mandatory
    try:
        from .templates_plugins import SYNC_SCRIPT_TEMPLATE
        
        sync_script_path = conport_db_dir / "aimed_sync.py"
        sync_needs_creation = True
        
        if sync_script_path.exists():
            try:
                # Check if file is valid (not empty or corrupted)
                if sync_script_path.stat().st_size > 100:
                    sync_needs_creation = False
                    log.debug(f"aimed_sync.py exists and is valid at {sync_script_path}")
                else:
                    log.info(f"aimed_sync.py exists but is empty or corrupted. Recreating at {sync_script_path}")
            except Exception as e:
                log.warning(f"Failed to check sync script file size: {e}. Recreating script.")
        
        if sync_needs_creation:
            try:
                log.info(f"Creating aimed_sync.py (pluggable feature) at {sync_script_path}")
                
                # Determine workspace_id from sync script location (same as launcher)
                actual_workspace_id = conport_db_dir.parent.resolve().as_posix()
                
                with open(sync_script_path, 'w', encoding='utf-8') as f:
                    template = SYNC_SCRIPT_TEMPLATE.lstrip('\n')
                    
                    # Replace template variable (following Decision 107 pattern)
                    content = template.replace('{workspace_id}', actual_workspace_id)
                    f.write(content)
                    f.flush()
                
                log.info(f"Successfully created aimed_sync.py ({len(content)} bytes)")
                
                # Make it executable
                try:
                    os.chmod(sync_script_path, 0o755)
                except Exception as chmod_e:
                    log.warning(f"Failed to chmod +x for {sync_script_path}: {chmod_e}")
                    
            except Exception as e:
                log.error(f"Failed to create aimed_sync.py at {sync_script_path}: {e}", exc_info=True)
                # Clean up failed file
                try:
                    if sync_script_path.exists():
                        sync_script_path.unlink()
                        log.debug(f"Cleaned up failed sync script file at {sync_script_path}")
                except Exception:
                    pass
    except ImportError:
        # templates_plugins.py doesn't exist - this is normal/expected
        # Sync feature is not available, but core AIMED works fine
        log.debug("Sync feature not available (templates_plugins.py not found) - this is expected for core-only installations")

def run_migrations(db_path: Path):
    """
    Runs Alembic migrations to upgrade the database to the latest version.
    This function is called on database connection to ensure schema is up-to-date.
    Alembic files are expected to be in the same directory as the database file.
    """
    # The directory where alembic.ini and alembic/ scripts are expected to be,
    # which is now the same directory as the database file.
    alembic_config_and_scripts_dir = db_path.parent

    alembic_ini_path = alembic_config_and_scripts_dir / "alembic.ini"
    alembic_scripts_path = alembic_config_and_scripts_dir / "alembic"

    # Initialize Alembic Config with the path to alembic.ini
    log.debug(f"Alembic: Current working directory (os.getcwd()): {os.getcwd()}")
    log.debug(f"Alembic: Initializing Config with alembic_ini_path = {alembic_ini_path.resolve()}")
    log.debug(f"Alembic: Setting script_location to alembic_scripts_path = {alembic_scripts_path.resolve()}")
    alembic_cfg = Config(str(alembic_ini_path))
    
    # Explicitly set the script location as a main option.
    # This is often more robust than relying on the .ini file or cmd_opts for this specific setting.
    alembic_cfg.set_main_option("script_location", alembic_scripts_path.as_posix())

    # Override sqlalchemy.url in alembic.ini to point to the specific workspace's DB
    # This is crucial for multi-workspace support.
    alembic_cfg.set_main_option("sqlalchemy.url", f"sqlite:///{db_path.as_posix()}")

    # Configure logging for Alembic (optional, can be done via Python's root logger)
    # The fileConfig call was causing issues and is not strictly necessary if alembic.ini
    # is only used for script_location and sqlalchemy.url.
    # Alembic's command.upgrade will handle its own logging if not explicitly configured.

    log.debug(f"Alembic Config: script_location = {alembic_cfg.get_main_option('script_location')}")
    log.debug(f"Alembic Config: sqlalchemy.url = {alembic_cfg.get_main_option('sqlalchemy.url')}")

    # Add explicit path existence check
    resolved_script_path = Path(alembic_cfg.get_main_option('script_location'))
    log.debug(f"Alembic: Resolved script path for existence check: {resolved_script_path}")
    if not resolved_script_path.exists():
        log.error(f"Alembic: CRITICAL - Script directory {resolved_script_path} does NOT exist according to Python!")
        raise DatabaseError(f"Alembic scripts directory not found: {resolved_script_path}")
    else:
        log.info(f"Alembic: Script directory {resolved_script_path} confirmed to exist by Python.")

    log.info(f"Running Alembic migrations for database: {db_path}")
    try:
        command.upgrade(alembic_cfg, "head")
        log.info(f"Alembic migrations completed successfully for {db_path}.")
    except Exception as e:
        log.error(f"Alembic migration failed for {db_path}: {e}", exc_info=True)
        raise DatabaseError(f"Database migration failed: {e}")



# --- Public API Imports ---
# Import functions from submodules to make them available at the top-level of the 'database' module.
# This allows other parts of the application to continue using `from .database import get_decisions`, etc.
from ._contexts import (
    get_product_context,
    update_product_context,
    get_active_context,
    update_active_context,
    search_context_fts,
    get_item_history,
)
from ._decisions import (
    log_decision,
    get_decisions,
    search_decisions_fts,
    update_decision_by_id,
    delete_decision_by_id,
)
from ._progress import (
    log_progress,
    get_progress,
    update_progress_entry,
    delete_progress_entry_by_id,
    search_progress_fts,
)
from ._patterns import (
    log_system_pattern,
    get_system_patterns,
    delete_system_pattern_by_id,
    search_system_patterns_fts,
)
from ._custom_data import (
    log_custom_data,
    get_custom_data,
    delete_custom_data,
    search_project_glossary_fts,
    search_custom_data_value_fts,
)
from ._links import (
    log_context_link,
    get_context_links,
    update_context_link,
    delete_context_link_by_id,
)
from ._reports import (
    get_recent_activity_summary_data,
    get_items_by_references,
)

# moved to _reports.py

# --- Cleanup ---
# Consider using a context manager or atexit to ensure connections are closed
import atexit
atexit.register(close_all_connections)

